{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbG_QnLnrSQZ"
   },
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "\n",
    "import torch\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19781,
     "status": "ok",
     "timestamp": 1684947344540,
     "user": {
      "displayName": "Dayal",
      "userId": "08303688375445058607"
     },
     "user_tz": 240
    },
    "id": "hO49W5e7s7CO",
    "outputId": "4a76bf4b-2f38-43e6-a71e-bdf91f907f64"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi -L\n",
    "# !pip install -q jax flax optax ml_collections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdnrHfYzrZTE"
   },
   "source": [
    "### Optional: Load Google drive and change directory to neurips_2023_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23706,
     "status": "ok",
     "timestamp": 1684947368239,
     "user": {
      "displayName": "Dayal",
      "userId": "08303688375445058607"
     },
     "user_tz": 240
    },
    "id": "K9fG7CRcqeFs",
    "outputId": "946d087f-098a-4f2b-f7a4-c6eac80da857"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running on Google Colab\")\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/neurips_2023_15410/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bheorS64sJci"
   },
   "source": [
    "### Load libraries\n",
    "\n",
    "Requirements: data_utils_pytorch, model_utils, train_mse_utils / train_xent_utils in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Us5F8JggsrYl"
   },
   "outputs": [],
   "source": [
    "loss = 'mse'\n",
    "# custom modules\n",
    "import data_utils_pytorch\n",
    "from jax_privacy.src.dp_sgd.gradients import GradientComputer, DpsgdGradientComputer\n",
    "import model_utils as models\n",
    "\n",
    "if loss == 'mse':\n",
    "    import train_mse_utils as train_utils\n",
    "elif loss == 'xent':\n",
    "    import train_xent_utils as train_utils\n",
    "else:\n",
    "    print('Unsupported loss function')\n",
    "    raise\n",
    "\n",
    "\n",
    "# in use imports\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "from ml_collections import config_dict\n",
    "\n",
    "#usual imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size':15})\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLV1eiF3sZ38"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RKPrPg0Xz-FF"
   },
   "outputs": [],
   "source": [
    "def poly(coeffs, x):\n",
    "    \"Given polynomial coefficients coeffs, evaluates the polynomal f(x)\"\n",
    "    degree = len(coeffs)-1\n",
    "    output = 0\n",
    "    for i in range(len(coeffs)):\n",
    "        output += coeffs[i] * x**(degree-i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrmS7Uj80LmX"
   },
   "source": [
    "pandas helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wzu3T4Su0Mf4"
   },
   "outputs": [],
   "source": [
    "def get_rows_where_col_equals(df, col, value):\n",
    "\treturn df.loc[df[col] == value].copy()\n",
    " \n",
    "def get_rows_where_col_in(df, col, values):\n",
    "\treturn df.loc[df[col].isin(values)].copy()\n",
    "\n",
    "def get_rows_where_col_greater(df, col, value):\n",
    "\treturn df.loc[df[col] > value].copy()\n",
    "\n",
    "def get_rows_where_col_less(df, col, value):\n",
    "\treturn df.loc[df[col] < value].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtzDAFWosw5y"
   },
   "source": [
    "### Model definition\n",
    "\n",
    "In this demo, we will use a 8 layer ReLU FCN to obtain the phase diagram of early training. The model can be changed to Myrtle CNNs, ResNets or linear FCNs using the model_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9PNmudd5tgak"
   },
   "outputs": [],
   "source": [
    "def create_train_state(config, batches):\n",
    "    \"\"\"\n",
    "    Description: Creates a Flax train state with learning rate eta = c / sharpness init\n",
    "    Input: \n",
    "        1. config: ml_collections dict which contains the model and optimizer hyperparameters\n",
    "        2. batches: batches used for sharpness estimation\n",
    "    Output:\n",
    "        1. state: Flax state with learning rate eta = c / sharpness init\n",
    "        2. sharpness_init: sharpness at initialization\n",
    "    \"\"\"\n",
    "\n",
    "    #create model\n",
    "    model = models.Myrtle(num_filters=config.width,\n",
    "    num_layers=config.depth,\n",
    "    num_classes=config.num_classes,\n",
    "    use_bias=config.use_bias,\n",
    "    varw=config.varw\n",
    "    )\n",
    "\n",
    "    #initialize the parameters\n",
    "    example = jax.random.normal(config.init_rng, shape = config.in_dim)\n",
    "    init_params = model.init(config.init_rng, example)['params'] #initialize parameters\n",
    "\n",
    "    #create a dummy state and optimizer for sharpness calculation\n",
    "    _opt = optax.sgd(learning_rate = 0.1, momentum = config.momentum)\n",
    "    _state = train_utils.TrainState.create(apply_fn = model.apply, params = init_params, opt = _opt)\n",
    "\n",
    "    #calculate hessian and learning rate\n",
    "    sharpness_init = train_utils.estimate_hessian(_state, batches, num_batches = config.measure_batches, power_iterations = config.power_iterations)\n",
    "    lr_rate = config.lr_const/sharpness_init\n",
    "\n",
    "    # create a new state with learning rate lr_rate; \n",
    "    # updating learning rate does not work since the recemt flax update\n",
    "    opt = optax.sgd(learning_rate = lr_rate, momentum = config.momentum)\n",
    "    state = train_utils.TrainState.create(apply_fn = model.apply, params = init_params, opt = opt)\n",
    "    return state, sharpness_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EToG5ukut2zg"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mMEaFZWKvRX5"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import jit\n",
    "\n",
    "\n",
    "# @partial(jit, static_argnames=['gradient_computer'])\n",
    "# def train_and_evaluate(lr_const, batch_size, num_steps, num_epochs, width, depth, sgd_rng, init_rng, train_ds, gradient_computer):\n",
    "#     \"\"\"\n",
    "#     Description: Creates a training state and trains for 10 steps\n",
    "#     Input: \n",
    "#         - Dynamically updated hyperparameters (lr_const, batch_size, etc.)\n",
    "#         - Other static arguments (gradient_computer)\n",
    "#     Output:\n",
    "#         - divergence: bool\n",
    "#         - train_results: np array\n",
    "#     \"\"\"\n",
    "#     train_results = list()\n",
    "\n",
    "#     # Create train and validation batches using rng\n",
    "#     rng = sgd_rng\n",
    "#     print(f\"train_ds[0] type: {type(train_ds[0])}, train_ds[1] type: {type(train_ds[1])}\")\n",
    "\n",
    "#     train_batches = data_utils_pytorch.data_stream(rng, train_ds, batch_size)\n",
    "#     # for _ in range(1):\n",
    "#     #     batch = next(train_batches)\n",
    "#     #     print(f\"Batch x type: {type(batch[0])}, Batch y type: {type(batch[1])}\")\n",
    "\n",
    "#     # Create model\n",
    "#     configNew = {\"width\": width, \"depth\": depth, \"lr_const\": lr_const}  # Create minimal config if needed\n",
    "#     state, sharpness_init = create_train_state(configNew, train_batches)\n",
    "\n",
    "#     # Measure metrics at initialization\n",
    "#     train_loss_init, train_acc_init = train_utils.measure_state(state, train_batches, num_steps, batch_size)\n",
    "\n",
    "#     # Store results at init\n",
    "#     result_init = jnp.asarray([0, 0, train_loss_init, train_loss_init, train_acc_init, sharpness_init, sharpness_init])\n",
    "#     train_results.append(result_init)\n",
    "#     print(f'{0}, {0}, {train_loss_init:0.4f}, {train_acc_init:0.4f}, {sharpness_init:0.4f}, {sharpness_init:0.4f}')\n",
    "\n",
    "#     divergence = False\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         rng, _ = jax.random.split(rng)\n",
    "#         batches = data_utils_pytorch.data_stream(rng, train_ds, batch_size)\n",
    "\n",
    "#         for batch_ix in range(num_steps):\n",
    "#             batch = next(batches)\n",
    "#             step = num_steps * epoch + batch_ix\n",
    "\n",
    "#             # Compute gradient and update the model\n",
    "#             state, loss_batch = train_utils.train_batch(state, batch, gradient_computer, batch_size)\n",
    "\n",
    "#             if loss_batch > 10**5:\n",
    "#                 divergence = True\n",
    "#                 break\n",
    "\n",
    "#             # Evaluate training loss, accuracy, and sharpness at step t\n",
    "#             train_loss_step, train_acc_step = train_utils.measure_state(state, train_batches, num_steps, batch_size)\n",
    "#             sharpness_step = train_utils.estimate_hessian(state, batches, num_batches=batch_size, power_iterations=10)\n",
    "\n",
    "#             # Store results\n",
    "#             result_step = jnp.asarray([step+1, epoch+1, train_loss_step, train_loss_init, train_acc_step, sharpness_step, sharpness_init])\n",
    "#             train_results.append(result_step)\n",
    "#             print(f'{step+1}, {epoch+1}, {train_loss_step:0.4f}, {train_acc_step:0.4f}, {sharpness_step:0.4f}, {sharpness_init:0.4f}')\n",
    "\n",
    "#     del state\n",
    "#     train_results = jnp.asarray(train_results)\n",
    "#     train_results = jax.device_get(train_results)\n",
    "#     return divergence, train_results\n",
    "\n",
    "\n",
    "@partial(jit, static_argnames=['gradient_computer'])\n",
    "def train_and_evaluate(config, train_ds, gradient_computer):\n",
    "    \"\"\"\n",
    "    Description: Creates a training state and trains for 10 steps\n",
    "    Input: \n",
    "        1. config: ml_collections dictionary containing all the hyperparameters\n",
    "        2. train_ds: tuple (x_train, y_train) of the training data\n",
    "    Output: \n",
    "        1. Divergence: bool; Flag for training divergence\n",
    "        2. train_results: np array containing training loss, sharpness and accuracy trajectories\n",
    "    \"\"\"\n",
    "\n",
    "    train_results = list()\n",
    "\n",
    "    # create train and validation batches using rng\n",
    "    rng = config.sgd_rng\n",
    "    train_batches = data_utils_pytorch.data_stream(rng, train_ds, config.measure_examples)\n",
    "\n",
    "    # create model\n",
    "    state, sharpness_init = create_train_state(config, train_batches)\n",
    "    init_params = state.params\n",
    "\n",
    "    # measure metrics at initialization\n",
    "    train_loss_init, train_acc_init = train_utils.measure_state(state, train_batches, config.num_train, config.measure_examples)\n",
    "\n",
    "    # store results at init\n",
    "    result_init = jnp.asarray([0, 0, train_loss_init, train_loss_init, train_acc_init, sharpness_init, sharpness_init])\n",
    "    train_results.append(result_init)\n",
    "    print(f'{0}, {0}, {train_loss_init:0.4f}, {train_acc_init:0.4f}, {sharpness_init:0.4f}, {sharpness_init:0.4f}')\n",
    "\n",
    "    divergence = False\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        rng, _ = jax.random.split(rng)\n",
    "        batches = data_utils_pytorch.data_stream(rng, train_ds, config.batch_size)\n",
    "\n",
    "        for batch_ix in range(config.num_steps):\n",
    "            batch = next(batches)\n",
    "            x, y = batch\n",
    "            step = config.num_batches * epoch + batch_ix\n",
    "            \n",
    "            # compute gradient and update the model\n",
    "            state, loss_batch = train_utils.train_batch(state, batch, gradient_computer, config.batch_size)\n",
    "\n",
    "            # hard cut-off on loss divergence\n",
    "            if loss_batch > 10**5: \n",
    "                divergence = True\n",
    "                break\n",
    "\n",
    "            # evaluate training loss, accuracy, and sharpness at step t            \n",
    "            train_loss_step, train_acc_step = train_utils.measure_state(state, train_batches, config.num_train, config.measure_examples)\n",
    "            sharpness_step = train_utils.estimate_hessian(state, batches, num_batches=config.measure_batches, power_iterations=config.power_iterations)\n",
    "    \n",
    "            # store results\n",
    "            result_step = jnp.asarray([step+1, epoch+1, train_loss_step, train_loss_init, train_acc_step, sharpness_step, sharpness_init])\n",
    "            train_results.append(result_step)\n",
    "            print(f'{step+1}, {epoch+1}, {train_loss_step:0.4f}, {train_acc_step:0.4f}, {sharpness_step:0.4f}, {sharpness_init:0.4f}')\n",
    "    \n",
    "    del state\n",
    "    train_results = jnp.asarray(train_results)\n",
    "    train_results = jax.device_get(train_results)\n",
    "    return divergence, train_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRJpM7cTuOdl"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bYcCxrDHxXgT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train_ds[0] type: <class 'jaxlib.xla_extension.ArrayImpl'>, train_ds[1] type: <class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "\n",
    "train_ds, test_ds, info = data_utils_pytorch.load_data_pytorch(dataset, num_classes)\n",
    "\n",
    "# Debug to confirm types\n",
    "print(f\"train_ds[0] type: {type(train_ds[0])}, train_ds[1] type: {type(train_ds[1])}\")\n",
    "\n",
    "#Hyperparameters\n",
    "config = config_dict.ConfigDict()\n",
    "config.num_train, config.num_test = info.num_train, info.num_test\n",
    "\n",
    "config.act = 'relu'\n",
    "config.model = f'fc_{config.act}'\n",
    "config.use_bias = False \n",
    "config.varw = 2.0 # this is the variance of all but last layer\n",
    "config.varwL = 1.0 # last layer variance\n",
    "\n",
    "widths = [128, 256, 512]\n",
    "config.depth = 5\n",
    "config.in_dim = info.in_dim\n",
    "config.num_classes = info.num_classes \n",
    "\n",
    "# batch size\n",
    "config.batch_size = 128\n",
    "# batch size for hessian measurement\n",
    "config.measure_examples = 512\n",
    "#number of batches for hessian measurement\n",
    "config.measure_batches = 1\n",
    "\n",
    "config.num_batches = data_utils_pytorch.estimate_num_batches(config.num_train, config.batch_size)\n",
    "\n",
    "#number of power iterations\n",
    "config.power_iterations = 20\n",
    "\n",
    "#optimizer related hyperparameters\n",
    "# Sample learning rates in powers of 2 starting with 2**0.0\n",
    "lr_exp_start = jax.device_put(0.0)\n",
    "lr_step = 0.2 # step size for increasing the learning rate\n",
    "config.momentum = jax.device_put(0.0) # momentum for sgd\n",
    "config.num_steps = 10 # train for ten steps\n",
    "config.num_epochs = 1 # one epoch only\n",
    "\n",
    "# averages\n",
    "init_averages = 1 #number of initialization averages\n",
    "sgd_runs = 1 # number of sgd runs for each initialization\n",
    "\n",
    "\n",
    "gradient_computer = DpsgdGradientComputer(\n",
    "    clipping_norm=1.0,             # Set the clipping norm\n",
    "    noise_multiplier=0.5,          # Set the noise multiplier for DP\n",
    "    rescale_to_unit_norm=True,     # Normalize clipped gradients\n",
    "    vectorize_grad_clipping=True   # Optimize clipping process\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQdxQDfkudVk"
   },
   "source": [
    "### Early time experiment:\n",
    "\n",
    "* We will train a 8 layer ReLU FCN for 10 steps using MSE loss using SGD with learning rate $\\eta = c / \\lambda_0^H$ and batch size $B = 512$. Here, $\\lambda_0^H$ is the sharpness at initialization. The same experiment can be performed with other architectures by replacing the model definition in the create_train_state function\n",
    "\n",
    "* We will sample learning rate constants $c$ in powers of 2, i.e., $c = 2^x$ with $x$ incremented in steps of $0.1$. \n",
    "\n",
    "* We will start with $x = x_{min} = 0.0$, wich corresponds to $\\eta = 1 / \\lambda_0^H$, as we are interested in the catapult dynamics.\n",
    "\n",
    "* To reduce computational time, we will run the experiment for only one random initialization.\n",
    "\n",
    "\n",
    "Note: This experiment takes about 45 mins to run on a V100 with 16 GB RAM. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2111853,
     "status": "ok",
     "timestamp": 1684873580662,
     "user": {
      "displayName": "Dayal",
      "userId": "08303688375445058607"
     },
     "user_tz": 240
    },
    "id": "y4-eRUbixSiE",
    "outputId": "079bdc97-ad79-41ea-c323-a7f0b023d9e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 512, d: 5, I: 1, J: 1, x: 0.0, B: 128, t: 10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error interpreting argument to <function train_and_evaluate at 0x7f4ba4fe80e0> as an abstract array. The problematic value is of type <class 'str'> and was passed to the function at path config['act'].\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/jaxenv/lib/python3.12/site-packages/jax/_src/api_util.py:604\u001b[0m, in \u001b[0;36mshaped_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    603\u001b[0m handler \u001b[38;5;241m=\u001b[39m _shaped_abstractify_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtype\u001b[39m(x), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m _shaped_abstractify_slow(x)\n",
      "File \u001b[0;32m~/jaxenv/lib/python3.12/site-packages/jax/_src/api_util.py:610\u001b[0m, in \u001b[0;36m_str_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str_abstractify\u001b[39m(x):\n\u001b[0;32m--> 610\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'relu' of type <class 'str'> is not a valid JAX type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m config\u001b[38;5;241m.\u001b[39mlr_const \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlr_exp\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, d: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, I: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, J: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, x: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_exp\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, B: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, t: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mnum_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m divergence, train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_computer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m divergence:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# append training results\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_results, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_step\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss_init\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharpness_step\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msharpness_init\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/jaxenv/lib/python3.12/site-packages/jax/_src/pjit.py:622\u001b[0m, in \u001b[0;36m_infer_params_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    620\u001b[0m       arg_description \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdbg\u001b[38;5;241m.\u001b[39marg_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dbg\n\u001b[1;32m    621\u001b[0m                          \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflattened argument number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 622\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError interpreting argument to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The problematic value is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and was passed to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the function at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis typically means that a jit-wrapped function was called with a non-array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument, and this argument was not marked as static using the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m static_argnums or static_argnames parameters of jax.jit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m       ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    631\u001b[0m   in_type \u001b[38;5;241m=\u001b[39m in_avals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(avals)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Error interpreting argument to <function train_and_evaluate at 0x7f4ba4fe80e0> as an abstract array. The problematic value is of type <class 'str'> and was passed to the function at path config['act'].\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit."
     ]
    }
   ],
   "source": [
    "# # for storing trajectories\n",
    "# dfs = list()\n",
    "\n",
    "# # run for all widths\n",
    "# for width in widths:\n",
    "#     config.width = width\n",
    "#     # run for init_averages different initializations\n",
    "#     for iteration in range(1, init_averages + 1):\n",
    "#         config.init_rng = jax.random.PRNGKey(iteration)\n",
    "#         # run for sgd_run different mini batch sequences\n",
    "#         for run in range(1, sgd_runs + 1):\n",
    "#             config.sgd_rng = jax.random.PRNGKey(run)\n",
    "#             lr_exp = lr_exp_start\n",
    "#             divergence = False\n",
    "\n",
    "#             while not divergence:\n",
    "#                 # Extract dynamically changing values from config\n",
    "#                 lr_const = 2**lr_exp\n",
    "#                 sgd_rng = config.sgd_rng\n",
    "#                 init_rng = config.init_rng\n",
    "#                 batch_size = config.batch_size\n",
    "#                 num_steps = config.num_steps\n",
    "#                 width = config.width\n",
    "#                 depth = config.depth\n",
    "#                 num_epochs = config.num_epochs\n",
    "\n",
    "#                 print(f'w: {width}, d: {depth}, I: {iteration}, J: {run}, x: {lr_exp:0.1f}, B: {batch_size}, t: {num_steps}')\n",
    "                \n",
    "\n",
    "    \n",
    "\n",
    "#                 print(f\"train_ds[0] type: {type(train_ds[0])}, train_ds[1] type: {type(train_ds[1])}\")\n",
    "\n",
    "#                 # Call the function with dynamic values\n",
    "#                 divergence, train_results = train_and_evaluate(\n",
    "#                     lr_const, batch_size, num_steps, num_epochs, width, depth, sgd_rng, init_rng, train_ds, gradient_computer\n",
    "#                 )\n",
    "\n",
    "#                 if not divergence:\n",
    "#                     # Append training results\n",
    "#                     df = pd.DataFrame(\n",
    "#                         train_results,\n",
    "#                         columns=[\n",
    "#                             'step', 'epoch', 'train_loss_step', 'train_loss_init', 'train_accuracy', 'sharpness_step', 'sharpness_init'\n",
    "#                         ],\n",
    "#                         dtype=float,\n",
    "#                     )\n",
    "#                     df['lr_exp'] = lr_exp\n",
    "#                     df['lr_const'] = lr_const\n",
    "#                     df['batch_size'] = batch_size\n",
    "#                     df['num_steps'] = num_epochs\n",
    "#                     df['I'] = iteration\n",
    "#                     df['J'] = run\n",
    "#                     df['width'] = width\n",
    "#                     df['depth'] = depth\n",
    "#                     dfs.append(df)\n",
    "#                     del df\n",
    "\n",
    "#                 else:\n",
    "#                     print('Divergence')\n",
    "\n",
    "#                 del train_results\n",
    "\n",
    "#                 lr_exp += lr_step\n",
    "\n",
    "#                 collected = gc.collect()\n",
    "\n",
    "# # Concatenate results\n",
    "# dfs = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "# for storing trajectories\n",
    "dfs = list()\n",
    "\n",
    "#run for all widths\n",
    "for width in widths:\n",
    "    config.width = 512\n",
    "    #run for init_averages different initializations\n",
    "    for iteration in range(1, init_averages+1):\n",
    "        config.init_rng = jax.random.PRNGKey(iteration)\n",
    "        #run for sgd_run different mini batch sequences\n",
    "        for run in range(1, sgd_runs + 1):\n",
    "            config.sgd_rng = jax.random.PRNGKey(run)\n",
    "            lr_exp = lr_exp_start\n",
    "            divergence = False\n",
    "\n",
    "            while not divergence:\n",
    "                config.lr_exp = lr_exp\n",
    "                config.lr_const = 2**lr_exp\n",
    "                print(f'w: {config.width}, d: {config.depth}, I: {iteration}, J: {run}, x: {lr_exp:0.1f}, B: {config.batch_size}, t: {config.num_steps}')\n",
    "\n",
    "                divergence, train_results = train_and_evaluate(config.to_dict(), train_ds, gradient_computer)\n",
    "\n",
    "                if not divergence:\n",
    "                    # append training results\n",
    "                    df = pd.DataFrame(train_results, columns = ['step', 'epoch', 'train_loss_step', 'train_loss_init', 'train_accuracy', 'sharpness_step', 'sharpness_init'], dtype = float)\n",
    "                    df['lr_exp'] = config.lr_exp; df['lr_const'] = config.lr_const; df['batch_size'] = config.batch_size; df['num_steps'] = config.num_epochs; df['I'] = iteration; df['J'] = run; df['width'] = config.width; df['depth'] = config.depth\n",
    "                    dfs.append(df)\n",
    "                    del df\n",
    "\n",
    "                else:\n",
    "                    print('Divergence')\n",
    "\n",
    "                del train_results\n",
    "\n",
    "                lr_exp += lr_step\n",
    "\n",
    "                collected = gc.collect()\n",
    "                #print(\"Garbage collector: collected %d objects.\" % (collected))\n",
    "\n",
    "# concatenate results\n",
    "dfs = pd.concat(dfs, axis = 0, ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKW6PqQ1y4Ak"
   },
   "source": [
    "Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uUMiK4dy6Kw"
   },
   "outputs": [],
   "source": [
    "# early training dynamics\n",
    "dfs = dfs.astype(float)\n",
    "dfs['norm_sharp'] = dfs['sharpness_step'] / dfs['sharpness_init']\n",
    "dfs['norm_loss'] = dfs['train_loss_step'] / dfs['train_loss_init']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU_fOJmBiunD"
   },
   "source": [
    "### Plot training trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2974,
     "status": "ok",
     "timestamp": 1684873583634,
     "user": {
      "displayName": "Dayal",
      "userId": "08303688375445058607"
     },
     "user_tz": 240
    },
    "id": "3a92CEEziz18",
    "outputId": "5cf506f1-d3ee-438f-ce92-b19a203eef55"
   },
   "outputs": [],
   "source": [
    "lr_plot = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "dfs['lr_exp'] = dfs['lr_exp'].round(1)\n",
    "dfs['lr_const'] = dfs['lr_const'].round(1)\n",
    "\n",
    "df_plot = get_rows_where_col_in(dfs, 'lr_exp', lr_plot)\n",
    "print(df_plot['lr_exp'].unique())\n",
    "\n",
    "for i, width in enumerate(widths):\n",
    "    \n",
    "    df_width = get_rows_where_col_equals(df_plot, 'width', width)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n",
    "    ax = axes[0]\n",
    "    ax = sns.lineplot(x = 'step', y = 'train_loss_step', data = df_width, hue = 'lr_const', palette = 'crest', legend = 'full', ax = ax)\n",
    "    ax.set_xlabel('step')\n",
    "    ax.set_ylabel(r'Training loss')\n",
    "    ax.set_title(f'width: {width}')\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax = sns.lineplot(x = 'step', y = 'norm_sharp', data = df_width, hue = 'lr_const', palette = 'crest', legend = 'full', ax = ax)\n",
    "    ax.set_xlabel('step')\n",
    "    ax.set_ylabel(r'$\\frac{\\lambda_t^H}{\\lambda_0^H}$')\n",
    "    ax.set_title(f'width: {width}')\n",
    "    ax.legend(title = r'$c$', loc='center left', bbox_to_anchor=(1, 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz88X27hAe3D"
   },
   "source": [
    "### Estimate critical constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kya_EBGDATye"
   },
   "outputs": [],
   "source": [
    "# note: For zero initialized networks, sharpness does not decrease appreciably during early training. \n",
    "# To avoid random fluctuations in sharpness to be considered as sharpness catapult, we compare the normalized sharpness with 1+epsilon to define sharpness catapult.\n",
    "\n",
    "epsilon = 0.05\n",
    "\n",
    "#estimate c_loss\n",
    "df_filtered = dfs[dfs['norm_loss'] > 1.0]\n",
    "df_grouped = df_filtered.groupby('width')['lr_const'].min()\n",
    "df_closs = df_grouped.reset_index().astype(float)\n",
    "df_closs['1w'] = 1 / df_closs['width']\n",
    "\n",
    "#estimate c_sharp\n",
    "df_filtered = dfs[dfs['norm_sharp'] > 1 + epsilon]\n",
    "df_grouped = df_filtered.groupby('width')['lr_const'].min()\n",
    "df_csharp = df_grouped.reset_index().astype(float)\n",
    "df_csharp['1w'] = 1 / df_csharp['width']\n",
    "\n",
    "#estimate c_max\n",
    "df_grouped = dfs.groupby('width')['lr_const'].max()\n",
    "df_cmax = df_grouped.reset_index().astype(float)\n",
    "df_cmax['1w'] = 1 / df_cmax['width']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVcp-xIRv5ib"
   },
   "source": [
    "### Plot the phase diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1684873584138,
     "user": {
      "displayName": "Dayal",
      "userId": "08303688375445058607"
     },
     "user_tz": 240
    },
    "id": "quXF4TeoNy3I",
    "outputId": "830f46de-22fd-4fda-db40-6cca53879660"
   },
   "outputs": [],
   "source": [
    "order = 1 #order of curve to fit\n",
    "# for this phase diagram order 1 works better\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (8, 6))\n",
    "colors = sns.color_palette('tab10', 3)\n",
    "\n",
    "ax.scatter('lr_const', '1w', data = df_closs, marker = 's', color = 'black', label = r'$\\langle c_{loss} \\rangle$', facecolors = 'none', zorder = 2, s = 64)\n",
    "ax.scatter('lr_const', '1w', data = df_csharp, marker = '^', color = 'black', label = r'$\\langle c_{sharp} \\rangle$', facecolors = 'none', zorder = 2, s = 64)\n",
    "ax.scatter('lr_const', '1w', data = df_cmax, marker = 'D', color = 'black', label = r'$\\langle c_{max} \\rangle$', facecolors = 'none', zorder = 2, s = 64)\n",
    "\n",
    "# fit curves\n",
    "y = df_closs['1w'].values\n",
    "\n",
    "ymin = np.min(y)\n",
    "ymax = np.max(y)\n",
    "\n",
    "y_new = np.array([2**i for i in np.arange(np.log2(ymin) - 0.1, np.log2(ymax) + 0.1, 0.05)])\n",
    "\n",
    "x = df_closs['lr_const'].values\n",
    "coeffs = np.polyfit(y, x, order)\n",
    "smooth_loss = poly(coeffs, y_new)\n",
    "ax.plot(smooth_loss, y_new, '-', color = 'black')\n",
    "\n",
    "x = df_csharp['lr_const'].values\n",
    "coeffs = np.polyfit(y, x, order)\n",
    "smooth_sharp = poly(coeffs, y_new)\n",
    "ax.plot(smooth_sharp, y_new, '-', color = 'black')\n",
    "\n",
    "\n",
    "x = df_cmax['lr_const'].values\n",
    "coeffs = np.polyfit(y, x, order)\n",
    "smooth_max = poly(coeffs, y_new)\n",
    "ax.plot(smooth_max, y_new, '-', color = 'black')\n",
    "\n",
    "x_max = np.max(x) + 5\n",
    "\n",
    "#Fill colors\n",
    "y1 = 0.0*np.ones(len(y_new))\n",
    "y2 = 2*np.ones(len(y_new))\n",
    "ax.fill(np.append(y1, y2[::-1]), np.append(y_new, y_new[::-1]), '#CFF5E7')\n",
    "\n",
    "y1 = 2*np.ones(len(y_new))\n",
    "y2 = smooth_loss\n",
    "ax.fill(np.append(y1, y2[::-1]), np.append(y_new, y_new[::-1]), '#FFFAD7')\n",
    "\n",
    "y1 = smooth_loss\n",
    "y2 = smooth_sharp\n",
    "ax.fill(np.append(y1, y2[::-1]), np.append(y_new, y_new[::-1]), '#FCDDB0')\n",
    "\n",
    "y1 = smooth_sharp\n",
    "y2 = smooth_max\n",
    "ax.fill(np.append(y1, y2[::-1]), np.append(y_new, y_new[::-1]), '#FF9F9F')\n",
    "\n",
    "y1 = smooth_max\n",
    "y2 = x_max*np.ones(len(y_new))\n",
    "ax.fill(np.append(y1, y2[::-1]), np.append(y_new, y_new[::-1]), '#E97777')\n",
    "\n",
    "\n",
    "ax.axvline(x = 2.0, linestyle = '--', color = 'gray')\n",
    "ax.set_xscale('log', base = 2)\n",
    "ax.set_yscale('log', base = 2)\n",
    "ax.legend(fontsize = 20, loc='upper left', facecolor = 'white', framealpha = 0.5)\n",
    "ax.set_ylabel(r'$1/w$')\n",
    "ax.set_xlabel(r'$c$')\n",
    "ax.set_xlim(0.5, x_max)\n",
    "ax.set_ylim(y_new[0], y_new[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJRET89awRW7"
   },
   "source": [
    "Key results: \n",
    "\n",
    "\n",
    "*   critical constants $c_{loss}, c_{sharp}$ do not scale with $1 / w$\n",
    "*   $c_{loss} = 2$ independent of depth and width."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNiR+DeEgZcx6P0gQLVdroR",
   "gpuType": "V100",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
